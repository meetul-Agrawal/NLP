{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9700a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28a774",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dfa02",
   "metadata": {},
   "source": [
    "Splitting the sentence into words or creating a list of words from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf052e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!', ',', 'This', 'is', 'Meet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence = 'Hello ! , This is Meet'\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30f98a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'allow', 'you', 'to', 'go', 'home', 'early']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "token = TreebankWordTokenizer()\n",
    "sentence = \"I can't allow you to go home early\"\n",
    "token.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4f04e",
   "metadata": {},
   "source": [
    "The most significant convention of a tokenizer is to separate contractions. For example, if we use word_tokenize() module for this purpose, it will give the output as follows −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634757c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', \"n't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"won't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcce6bd",
   "metadata": {},
   "source": [
    "Such kind of convention by TreebankWordTokenizer is unacceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e6334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', \"n't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "token = TreebankWordTokenizer()\n",
    "token.tokenize(\"won't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b2759",
   "metadata": {},
   "source": [
    "That’s why we have two alternative word tokenizers namely PunktWordTokenizer and WordPunctTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14eb09",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77914600",
   "metadata": {},
   "source": [
    "An alternative word tokenizer that splits all punctuation into separate tokens. Let us understand it with the following simple example −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abeee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', \"'\", 't', 'allow', 'you', 'to', 'go', 'home', 'early']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_token = WordPunctTokenizer()\n",
    "\n",
    "word_token.tokenize(\"I can't allow you to go home early\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6eac9",
   "metadata": {},
   "source": [
    "### Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ab2ef",
   "metadata": {},
   "source": [
    "An obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences. Suppose we need to count average words in sentences, how we can do this? For accomplishing this task, we need both sentence tokenization and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104e39fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let us understand the difference between sentence & word tokenizer.',\n",
       " 'It is going to be a simple example.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Let us understand the difference between sentence & word tokenizer. \n",
    "It is going to be a simple example.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7443f7",
   "metadata": {},
   "source": [
    "If you feel that the output of word tokenizer is unacceptable and want complete control over how to tokenize the text, we have regular expression which can be used while doing sentence tokenization. NLTK provide RegexpTokenizer class to achieve this.\n",
    "\n",
    "Let us understand the concept with the help of two examples below.\n",
    "\n",
    "In first example we will be using regular expression for matching alphanumeric tokens plus single quotes so that we don’t split contractions like “won’t”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "359d329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['won', 't', 'is', 'a', 'coincidence']\n",
      "['can', 't', 'is', 'a', 'coincidence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# we will be using regular expression to tokenize on whitespace.\n",
    "reg1 = RegexpTokenizer(\"[\\w]+\")\n",
    "print(reg1.tokenize(\"won't is a coincidence.\"))\n",
    "print(reg1.tokenize(\"can't is a coincidence .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d17f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"won't\", 'is', 'a', 'contraction.']\n"
     ]
    }
   ],
   "source": [
    "reg2 = RegexpTokenizer('\\s+' , gaps = True)\n",
    "print(reg2.tokenize(\"won't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26954bc",
   "metadata": {},
   "source": [
    "From the above output, we can see that the punctuation remains in the tokens. The parameter gaps = True means the pattern is going to identify the gaps to tokenize on. On the other hand, if we will use gaps = False parameter then the pattern would be used to identify the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0b10144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "reg2 = RegexpTokenizer('\\s+' , gaps = False)\n",
    "print(reg2.tokenize(\"won't is a contraction.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
